base_model: meta-llama/Llama-3.2-3B-Instruct

# Training dataset (with full reasoning traces)
datasets:
  - path: ./data/reasoning_trace_longest_quartile_1k_gpt_train.jsonl
    type: chat_template # implies data in the OpenAI format, see: https://docs.axolotl.ai/docs/dataset-formats/conversation.html
    train_on_eot: true

# eot_tokens: 
# - <|im_end|>

# Don't auto-split since we're providing explicit eval dataset
val_set_size: 0

# Evaluation dataset 
test_datasets:
  - path: ./data/reasoning_trace_longest_quartile_1k_gpt_val.jsonl
    type: chat_template
    split: train
    train_on_eot: true

output_dir: ./outputs/llama32_3B_reasoning_trace_longest_quartile_1k_gpt_2e-4_20250722_112829 # where to save final model
chat_template: alpaca


eval_batch_size: 1
seed: 3

load_in_4bit: true
adapter: lora
lora_r: 32
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules:
  - gate_proj
  - down_proj
  - up_proj
  - q_proj
  - v_proj
  - k_proj
  - o_proj
#batch_size: 16
gradient_accumulation_steps: 4
micro_batch_size: 1
# batch size = micro x accum x num_gpus

#for llama:
special_tokens:
  pad_token: <|eot_id|>

num_epochs: 10
# max_steps: 1000
warmup_ratio: 0.03
learning_rate: 2e-4
lr_scheduler: cosine
gradient_checkpointing: true
early_stopping_patience: 3
save_steps: 10
eval_steps: 10

sequence_len: 6402

optimizer: paged_adamw_8bit    

weight_decay: 0
seed: 1234

sample_packing: false # efficiency, but need to turn off if train split is small. (yours should be fine here?)
eval_sample_packing: false # efficiency, but need to turn off if val split is small (yours most likely will be, it gives cryptic errors if it's because val split is small so try running with and turning off it you get errors) ITS TOO SMALL
flash_attention: false # to use flash attention or not (depends if flash attention is installed/available for the gpu architecture you're using) NOT AVAILABLE
